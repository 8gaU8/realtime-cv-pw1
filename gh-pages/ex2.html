<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0" />
  <title>Realtime Image Processing: Practical works week1 Exercise 2</title>
  <style>
    html,
    body {
      width: 50%;
      height: 50%;
      margin: 0;
    }

    #c {
      width: 50%;
      height: 50%;
      display: block;
    }
  </style>
</head>

<body>
  <canvas id="c"></canvas>

  <script id="defaultVertexShader" type="x-shader/x-vertex">
    uniform mat4 modelViewMatrix;
    uniform mat4 projectionMatrix;

    precision highp float;

    in vec3 position;

    void main() {
        gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0 );
    }

  </script>

  <script id="anaglyphFragmentShader" type="x-shader/x-fragment">
    precision highp float;

    uniform int sizeDiv2;
    uniform sampler2D image;
    uniform float scale;
    uniform float translateX;
    uniform float translateY;

    out vec4 out_FragColor;

    vec3 trueAnaglyph(vec3 left, vec3 right){
        vec3 color = vec3(0.0);
        color.r = 0.299 * left.r + 0.587 * left.g + 0.114 * left.b;
        color.b = 0.299 * right.r + 0.587 * right.g + 0.114 * right.b;
        return color;
    }


    vec3 grayAnaglyph(vec3 left, vec3 right){
        vec3 color = vec3(0.0);
        color.r = 0.299 * left.r + 0.587 * left.g + 0.114 * left.b;
        color.g = 0.299 * right.r + 0.587 * right.g + 0.114 * right.b;
        color.b = 0.299 * right.r + 0.587 * right.g + 0.114 * right.b;
        return color;
    }

    vec3 colorAnaglyph(vec3 left, vec3 right) {
        vec3 color = vec3(0.0);
        color.r = left.r;
        color.g = right.g;
        color.b = right.b;
        return color;
    }

    vec3 halfColorAnaglyph(vec3 left, vec3 right) {
        vec3 color = vec3(0.0);
        color.r = 0.299 * left.r + 0.587 * left.g + 0.114 * left.b;
        color.g = right.g;
        color.b = right.b;
        return color;
    }

    vec3 optimizedAnaglyph(vec3 left, vec3 right) {
        vec3 color = vec3(0.0);
        color.r = 0.7 * left.r + 0.3 * left.r;
        color.g = right.g;
        color.b = right.b;
        return color;
    }


    void main(void) {
        ivec2 texSize2d = textureSize(image, 0);

        // Apply translation
        int leftX = int((gl_FragCoord.x + translateX * float(texSize2d.x))* scale ) % (texSize2d.x / 2);
        int leftY = int((gl_FragCoord.y + translateY * float(texSize2d.y))* scale ) % texSize2d.y;

        vec4 leftTextureValue = texelFetch(image, ivec2(leftX, leftY), 0 );

        int rightX = leftX + texSize2d.x / 2;
        int rightY = leftY;
        vec4 rightTextureValue = texelFetch(image, ivec2(rightX, rightY), 0 );

        // Color Anaglyphs
        out_FragColor.a = 1.0;
        // ANAGLYPH is macro defined at /src/shader.js
        out_FragColor.rgb = ANAGLYPH(leftTextureValue.rgb, rightTextureValue.rgb);

    }


  </script>

  <script type="importmap">
      {
        "imports": {
          "three": "https://unpkg.com/three@0.172.0/build/three.module.js",
          "three/addons/": "https://unpkg.com/three@0.172.0/examples/jsm/",
          "three/src/math/MathUtils": "https://unpkg.com/three@0.172.0/src/math/MathUtils.js",
          "three/examples/jsm/": "https://unpkg.com/three@0.172.0/examples/jsm/"
        }
      }
    </script>
  <script type="module">
    import * as THREE from "https://unpkg.com/three@0.172.0/build/three.module.js"
    import WEBGL from 'three/examples/jsm/capabilities/WebGL.js'
    import { OrbitControls } from 'three/examples/jsm/controls/OrbitControls.js'
    import { GUI } from 'three/examples/jsm/libs/lil-gui.module.min.js'


    // load shaders
    const defaultVertexShader = document.getElementById("defaultVertexShader").textContent
    const anaglyphFragmentShader = document.getElementById("anaglyphFragmentShader").textContent
    const anaglyphMacros = {
      trueAnaglyphs: { 'ANAGLYPH(left, right)': 'trueAnaglyph(left, right)' },
      colorAnaglyphs: { 'ANAGLYPH(left, right)': 'colorAnaglyph(left, right)' },
      grayAnaglyphs: { 'ANAGLYPH(left, right)': 'grayAnaglyph(left, right)' },
      halfColorAnaglyphs: {
        'ANAGLYPH(left, right)': 'halfColorAnaglyph(left, right)',
      },
      optimizedAnaglyphs: {
        'ANAGLYPH(left, right)': 'optimizedAnaglyph(left, right)',
      },
    }

    const anaglyphMacrosName = Object.keys(anaglyphMacros)


    /**
     * Utility functions for the application
     * Contains helper functions for Three.js scene management and event handling
     */

    /**
     * Creates a window resize event handler that updates camera and renderer when the window is resized
     * This ensures the scene maintains the correct aspect ratio and fills the viewport
     *
     * @param {THREE.Camera} camera - The camera to adjust aspect ratio for
     * @param {THREE.WebGLRenderer} renderer - The renderer to adjust size for
     * @returns {Function} A resize handler function that can be used as an event listener
     */
    const createWindowResizeHandler = (camera, renderer) =>
    {
      const handleResize = () =>
      {
        camera.aspect = window.innerWidth / window.innerHeight
        camera.updateProjectionMatrix()

        renderer.setSize(window.innerWidth, window.innerHeight)
      }
      return handleResize
    }

    /**
     * Removes an object from the scene by its name if it exists
     * Useful for cleaning up scene objects before replacing them
     *
     * @param {THREE.Scene} scene - The scene to remove the object from
     * @param {string} name - The name of the object to be removed
     * @returns {void}
     */
    const removeObjectByName = (scene, name) =>
    {
      const targetObject = scene.getObjectByName(name)
      if (targetObject) {
        scene.remove(targetObject)
        console.log('Object removed:', name)
      }
    }

    // Configuration for video files, including dimensions and paths
    const videoConfig = {
      'moon.mp4': {
        height: null,
        width: null,
        heightFactor: 1,
        widthFactor: 1,
        posY: 0.6,
        path: 'https://8gau8.github.io/realtime-cv-pw1/gh-pages/assets/moon.mp4',
      },
      'sf.mp4': {
        height: null,
        width: null,
        heightFactor: 1,
        widthFactor: 2,
        posY: 0.3,
        path: 'https://8gau8.github.io/realtime-cv-pw1/gh-pages/assets/sf.mp4',
      },
    }

    // VideoController class manages video playback and texture creation
    export class VideoController
    {
      constructor()
      {
        this.defaultVideoName = 'sf.mp4' // Default video to load
        this.videoName = null // Currently loaded video name
        this.video = null // HTML video element
        this.videoNames = Object.keys(videoConfig) // List of available video names
      }

      // Load the video and resolve when it's ready
      async load()
      {
        return new Promise((resolve, reject) =>
        {
          if (!this.video) {
            reject(new Error('Video not initialized'))
            return
          }

          this.video.onloadeddata = () =>
          {
            resolve(this.video)
            this.video.play()
          }
          this.video.onerror = (e) =>
          {
            reject(new Error('Failed to load video: ' + e.message))
          }
        })
      }

      // Set a new video by name and load it
      async setVideo(videoName)
      {
        if (this.videoName === videoName) {
          console.log('video reload')
          return
        }
        this.videoName = videoName

        this.video = document.createElement('video')
        this.video.src = videoConfig[this.videoName].path
        this.video.preload = 'auto'
        this.video.load()
        this.video.muted = true
        this.video.loop = true

        await this.load()
        videoConfig[this.videoName].height = this.video.videoHeight
        videoConfig[this.videoName].width = this.video.videoWidth
      }

      /**
       * Creates a Three.js VideoTexture from the video element
       * Configures texture parameters for optimal rendering
       * @returns {THREE.VideoTexture} Configured video texture ready for use in materials
       */
      getVideoTexture()
      {
        const videoTexture = new THREE.VideoTexture(this.video)
        videoTexture.minFilter = THREE.NearestFilter
        videoTexture.magFilter = THREE.NearestFilter
        videoTexture.generateMipmaps = false
        videoTexture.format = THREE.RGBAFormat
        return videoTexture
      }

      /**
       * Returns the configuration object for the currently loaded video
       * @returns {Object} videoConfig - Video configuration containing dimensions and position settings
       * @returns {number} videoConfig.width - Width of the video
       * @returns {number} videoConfig.height - Height of the video
       * @returns {number} videoConfig.widthFactor - Width scaling factor for display
       * @returns {number} videoConfig.heightFactor - Height scaling factor for display
       *
       */
      getVideoConfig()
      {
        return videoConfig[this.videoName]
      }

      /**
       * Toggles the play/pause state of the video
       * If video is paused, it will play, and vice versa
       * @returns {void}
       */
      togglePlayPause()
      {
        if (!this.video) return

        if (this.video.paused) {
          console.log('play')
          this.video.play()
        } else {
          console.log('pause')
          this.video.pause()
        }
      }

      /**
       * Adjusts the current playback time of the video
       * @param {number} timeDelta - Number of seconds to add (positive) or subtract (negative)
       * @returns {void}
       */
      adjustVideoTime(timeDelta)
      {
        if (!this.video) return

        const updatedTime = Math.min(
          Math.max(this.video.currentTime + timeDelta, 0),
          this.video.duration,
        )
        this.video.currentTime = updatedTime
      }

      /**
       * Check if the video is ready for playback
       * @returns {boolean} True if the video is ready, false otherwise
       */
      ready()
      {
        return this.video !== null
      }
    }



    /**
     * Initializes the WebGL2 renderer
     * Creates the necessary DOM elements and configures the renderer
     * @returns {THREE.WebGLRenderer|boolean} The configured renderer or false if WebGL2 is not available
     */
    const initRenderer = () =>
    {
      if (WEBGL.isWebGL2Available() === false) {
        document.body.appendChild(WEBGL.getWebGL2ErrorMessage())
        return false
      }

      // Create container for renderer
      const container = document.createElement('div')
      document.body.appendChild(container)

      // Setup WebGL2 canvas and context
      const canvas = document.createElement('canvas')
      const context = canvas.getContext('webgl2')
      document.body.appendChild(canvas)

      // Configure renderer with WebGL2 support
      const renderer = new THREE.WebGLRenderer({
        canvas: canvas,
        context: context,
      })
      renderer.autoClear = false
      renderer.setPixelRatio(window.devicePixelRatio)
      renderer.setSize(window.innerWidth, window.innerHeight)
      renderer.shadowMap.enabled = false

      container.appendChild(renderer.domElement)
      return renderer
    }

    /**
     * Initializes the camera for the scene
     * Creates a perspective camera with appropriate positioning
     * @returns {THREE.PerspectiveCamera} The configured camera
     */
    const initCamera = () =>
    {
      const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.001, 10)
      camera.position.z = 1.5

      return camera
    }

    /**
     * Rendering Pipeline Pass Module
     * Creates an individual rendering pass for processing images with shaders
     * Each pass takes an input texture, applies a shader, and renders to a target
     */

    class RenderingPipelinePass
    {
      /**
       * Creates a rendering pass with defined shaders and settings
       * @param {THREE.Texture} sourceTexture - Input texture for the pass
       * @param {number} targetWidth - Width of the render target
       * @param {number} targetHeight - Height of the render target
       * @param {Object} uniforms - Shader uniforms
       * @param {Object} defines - Shader preprocessor defines
       * @param {string} fragmentShader - GLSL fragment shader code
       */
      constructor(sourceTexture, targetWidth, targetHeight, uniforms, defines, fragmentShader)
      {
        // Initialize scene and camera for this render pass
        this.scene = new THREE.Scene()
        this.camera = new THREE.OrthographicCamera(-1, 1, 1, -1, 1 / Math.pow(2, 53), 1)

        // Configure render target with appropriate settings for image processing
        const options = {
          minFilter: THREE.NearestFilter,
          magFilter: THREE.NearestFilter,
          format: THREE.RGBAFormat,
          type: THREE.FloatType,
        }
        this.renderTarget = new THREE.WebGLRenderTarget(targetWidth, targetHeight, options)

        // Set up uniforms including the source texture
        this.uniforms = { ...uniforms }
        this.uniforms['image'] = { type: 't', value: sourceTexture }

        this.defines = defines
        this.fragmentShader = fragmentShader

        // Create the rendering components
        this._buildPipeline()
      }

      /**
       * Get the output texture from this render pass
       * @returns {THREE.Texture} The rendered texture
       */
      getTexture()
      {
        return this.renderTarget.texture
      }

      /**
       * Create the internal scene with a shader material and plane
       * @private
       */
      _buildPipeline()
      {
        // Create a material using the provided shaders and defines
        const imageProcessingMaterial = new THREE.RawShaderMaterial({
          uniforms: this.uniforms,
          vertexShader: defaultVertexShader,
          fragmentShader: this.fragmentShader,
          glslVersion: THREE.GLSL3,
          defines: this.defines,
        })

        // Create a full-screen plane to render the shader effect
        const positions = new Float32Array([-1, -1, 0, 1, -1, 0, 1, 1, 0, -1, -1, 0, 1, 1, 0, -1, 1, 0])
        const positionsAttribute = new THREE.BufferAttribute(positions, 3)
        const geom = new THREE.BufferGeometry()
        geom.setAttribute('position', positionsAttribute)
        const plane = new THREE.Mesh(geom, imageProcessingMaterial)
        this.scene.add(plane)
      }

      /**
       * Execute the render pass
       * @param {THREE.WebGLRenderer} renderer - The WebGL renderer
       */
      render(renderer)
      {
        renderer.setRenderTarget(this.renderTarget)
        renderer.render(this.scene, this.camera)
        renderer.setRenderTarget(null)
      }
    }

    /**
     * ImageProcessing class handles the processing pipeline for creating anaglyph effects.
     * It applies filters to video textures and creates a 3D anaglyph representation.
     */
    class ImageProcessing
    {
      /**
       * Creates a new image processing pipeline with filters and anaglyph effect
       *
       * @param {THREE.Texture} sourceTexture - The source video texture to process
       * @param {Object} uniforms - Shader uniforms for image processing
       * @param {Object} anaglyphDefine - Shader macro definition for anaglyph effect
       * @param {Object} videoConfig - Configuration parameters for the video
       * @param {number} videoConfig.width - Width of the video
       * @param {number} videoConfig.height - Height of the video
       * @param {number} videoConfig.widthFactor - Width scaling factor for display
       * @param {number} videoConfig.heightFactor - Height scaling factor for display
       */
      constructor(sourceTexture, uniforms, anaglyphDefine, videoConfig)
      {
        this.sourceTexture = sourceTexture
        this.uniforms = uniforms
        this.videoConfig = videoConfig

        const sourceVideoWidth = this.videoConfig.width
        const sourceVideoHeight = this.videoConfig.height

        // Anaglyph requires splitting the image into left and right views
        this.targetWidth = sourceVideoWidth / 2
        this.targetHeight = sourceVideoHeight



        // Create the final anaglyph processing pass
        const anaglyphPath = new RenderingPipelinePass(
          sourceTexture,
          this.targetWidth,
          this.targetHeight,
          this.uniforms,
          anaglyphDefine,
          anaglyphFragmentShader,
        )
        this.anaglyphPath = anaglyphPath
      }

      /**
       * Creates a Three.js plane with the processed video texture
       *
       * @returns {THREE.Mesh} A plane mesh with the processed video texture applied
       */
      createVideoPlane()
      {
        const hf = this.videoConfig.heightFactor
        const wf = this.videoConfig.widthFactor

        // Calculate aspect ratio based on the video dimensions and factors
        const aspectRatio = (this.targetHeight * hf) / (this.targetWidth * wf)

        const lastTexture = this.anaglyphPath.getTexture()

        // Create a plane for displaying the processed video
        const geometry = new THREE.PlaneGeometry(1, aspectRatio)
        const material = new THREE.MeshBasicMaterial({
          map: lastTexture,
          side: THREE.FrontSide,
        })
        const plane = new THREE.Mesh(geometry, material)
        plane.receiveShadow = false
        plane.castShadow = false
        return plane
      }

      /**
       * Renders the complete processing pipeline
       *
       * @param {THREE.WebGLRenderer} renderer - The WebGL renderer used to render the pipeline
       */
      render(renderer)
      {
        // Then render the anaglyph effect
        this.anaglyphPath.render(renderer)
      }
    }

    class ImageProcessingMaterialController
    {
      /**
       * Creates a controller for managing image processing operations on video
       * @param {THREE.Scene} scene
       * @param {VideoController} videoController
       */
      constructor(scene, videoController, nbFilter)
      {
        this.scene = scene

        /** @type {VideoController} */
        this.videoController = videoController
        this.nbFilter = nbFilter

        // Initialize with the first anaglyph method
        const selectedAnaglyph = Object.keys(anaglyphMacros)[0]
        this.anaglyphDefine = anaglyphMacros[selectedAnaglyph]

        // Initialize image planes
        this.imageObjectProcessed = null
        this.imageObjectOriginal = null

        // Setup uniform values for shaders
        this.uniforms = {
          scale: { type: 'f', value: 1.0 },
          translateX: { type: 'f', value: 0.0 },
          translateY: { type: 'f', value: 0.0 },
        }

        // Initialize with default video
        this.init(videoController.defaultVideoName)
      }

      init(videoName)
      {
        this.onVideoChange(videoName)
      }

      /**
       * Updates the processed video display with current filters applied
       */
      updateProcessedPlane()
      {
        const name = 'videoPlaneProcessed'
        const posY = -this.videoController.getVideoConfig().posY

        const imageProcessing = this._createPlane(name, posY)
        // Store the created image processing instance for later rendering
        this.imageObjectProcessed = imageProcessing
      }

      /**
       * Updates the original video display with only anaglyph effect
       * Creates a new plane with the original video texture using only the anaglyph effect without filters
       * This provides a comparison view to the processed version
       */
      updateOriginalPlane()
      {
        const name = 'videoPlaneOriginal'
        const posY = this.videoController.getVideoConfig().posY

        const imageProcessing = this._createPlane(name, posY)
        this.imageObjectOriginal = imageProcessing
      }

      /**
       * Creates a video plane with specified filters and position
       * @private
       * @param {string} name - Unique name for the mesh in the scene
       * @param {number} posY - Y position of the plane in the scene
       * @returns {ImageProcessing} The created image processing instance
       */
      _createPlane(name, posY)
      {
        // Remove any existing plane with this name to avoid duplicates
        removeObjectByName(this.scene, name)

        // Create a new image processing instance with the specified filters
        const imageProcessing = new ImageProcessing(
          this.videoController.getVideoTexture(),
          this.uniforms,
          this.anaglyphDefine,
          this.videoController.getVideoConfig(),
        )

        // Get the rendered plane and configure its properties
        const plane = imageProcessing.createVideoPlane()
        plane.name = name
        plane.position.y = posY
        this.scene.add(plane)

        return imageProcessing
      }

      /**
       * Handles video change event
       * Loads the new video and updates both the original and processed planes
       * @async
       * @param {string} videoName - Name of the video to switch to
       * @returns {Promise<void>}
       */
      async onVideoChange(videoName)
      {
        if (this.videoController.videoName === videoName) return

        console.log('videoName', videoName)
        await this.videoController.setVideo(videoName)

        this.sourceTexture = this.videoController.getVideoTexture()
        this.updateOriginalPlane()
        this.updateProcessedPlane()
      }

      /**
       * Handles anaglyph method change event
       * @param {string} value - Name of the anaglyph method to use
       */
      onAnaglyphChange(value)
      {
        const anaglyphDefine = anaglyphMacros[value]
        if (this.anaglyphDefine === anaglyphDefine) return

        this.anaglyphDefine = anaglyphDefine
        this.updateOriginalPlane()
        this.updateProcessedPlane()
      }

      /**
       * Renders both original and processed video planes
       * @param {THREE.WebGLRenderer} renderer - WebGL renderer
       */
      render(renderer)
      {
        if (this.videoController.ready()) {
          if (this.imageObjectProcessed) this.imageObjectProcessed.render(renderer)
          if (this.imageObjectOriginal) this.imageObjectOriginal.render(renderer)
        }
      }
    }

    /**
     * GUI Initialization Module
     * Sets up the user interface controls for manipulating video and image processing parameters
     */

    /**
     * Initialize the GUI for controlling video and image processing
     * @param {GUI} gui - GUI instance
     * @param {ImageProcessingMaterialController} materialCtrl - Image processing controller
     * @param {VideoController} videoCtrl - Video controller
     */
    function initGUI(gui, materialCtrl, videoCtrl)
    {
      // Video control GUI
      const videoFolder = gui.addFolder('Video Control')

      // Add play/pause button
      videoFolder
        .add(
          {
            pausePlay: () => videoCtrl.togglePlayPause(),
          },
          'pausePlay',
        )
        .name('Pause/Play video')

      // Add skip forward button
      videoFolder
        .add(
          {
            add10sec: () => videoCtrl.adjustVideoTime(+10),
          },
          'add10sec',
        )
        .name('Add 10 seconds')

      videoFolder
        .add(
          {
            sub10sec: () => videoCtrl.adjustVideoTime(-10),
          },
          'sub10sec',
        )
        .name('Back 10 seconds')

      // Add video selection dropdown
      videoFolder
        .add({ video: videoCtrl.defaultVideoName }, 'video', videoCtrl.videoNames)
        .name('Video')
        .onChange((videoName) =>
        {
          materialCtrl.onVideoChange(videoName)
        })

      // Transform parameters controls
      const paramFolder = gui.addFolder('Transform')
      paramFolder.add(materialCtrl.uniforms.scale, 'value', 0.1, 10).name('Scale')
      paramFolder.add(materialCtrl.uniforms.translateX, 'value', 0, 1).name('Translate X')
      paramFolder.add(materialCtrl.uniforms.translateY, 'value', 0, 1).name('Translate Y')

      // Anaglyph method selection folder
      const anaglyphGUI = gui.addFolder('Anaglyph')
      anaglyphGUI
        .add({ value: anaglyphMacrosName[0] }, 'value', anaglyphMacrosName)
        .name('Method')
        .onChange((name) =>
        {
          materialCtrl.onAnaglyphChange(name)
        })

    }


    async function main()
    {
      // Initialize Three.js components
      const renderer = initRenderer()
      const camera = initCamera()
      const scene = new THREE.Scene()

      // Set up orbit controls for interactive camera movement
      const orbitControls = new OrbitControls(camera, renderer.domElement)

      // Handle window resize
      window.addEventListener('resize', createWindowResizeHandler(camera, renderer), false)

      // Initialize video controller
      const videoController = new VideoController()

      // Initialize material controller for image processing
      const materialController = new ImageProcessingMaterialController(scene, videoController)

      // Setup GUI controls
      const rootGui = new GUI()
      initGUI(rootGui, materialController, videoController)

      /**
       * Animation loop that runs continuously for rendering
       */
      function animate()
      {
        requestAnimationFrame(animate)
        orbitControls.update()
        renderer.clear()
        materialController.render(renderer)
        renderer.render(scene, camera)
      }

      // Start animation loop
      animate()
    }

    main()




  </script>
  <!-- <script type="module" src="./main.js"></script> -->
</body>

</html>