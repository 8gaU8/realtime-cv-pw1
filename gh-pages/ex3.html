<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0" />
  <title>Realtime Image Processing: Practical works week1 Exercise 3</title>
  <style>
    html,
    body {
      width: 50%;
      height: 50%;
      margin: 0;
    }

    #c {
      width: 50%;
      height: 50%;
      display: block;
    }
  </style>
</head>

<body>
  <canvas id="c"></canvas>

  <script id="defaultVertexShader" type="x-shader/x-vertex">
    uniform mat4 modelViewMatrix;
    uniform mat4 projectionMatrix;

    precision highp float;

    in vec3 position;

    void main() {
        gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0 );
    }

  </script>


  <script id="filterFragmentShader" type="x-shader/x-fragment">
    precision highp float;
  
    uniform sampler2D image;
    uniform float scale;
    uniform float translateX;
    uniform float translateY;
    uniform int kernelSizeDiv2;
    uniform float sigma;
    uniform float laplacianFactor;

    out vec4 out_FragColor;

    #define PI 3.1415926
    #define MEDIAN_KERNEL_SIZE 49
        
    ivec2 clampCoord(ivec2 coord, int sizeX, int sizeY){
        if (coord.x < sizeX/2){
            return ivec2(clamp(coord.x, 0, sizeX/2 - 2), clamp(coord.y, 0, sizeY - 1));
        }
        return ivec2(clamp(coord.x, sizeX/2+1, sizeX - 1), clamp(coord.y, 0, sizeY - 1));

    }

    vec4 averageFilter(int centerX, int centerY, int sizeX, int sizeY){
        vec4 textureValue = vec4(0.0);
        for (int i = -kernelSizeDiv2; i <= kernelSizeDiv2; i++) {
            for (int j = -kernelSizeDiv2; j <= kernelSizeDiv2; j++){
                textureValue += texelFetch( image, ivec2(centerX + i, centerY + j), 0 );
            }
        }
        return textureValue / float(4 * kernelSizeDiv2 * (kernelSizeDiv2 + 4) + 1);
    }

    vec4 gaussianFilter(int centerX, int centerY, int sizeX, int sizeY){
        // https://pages.stat.wisc.edu/~mchung/teaching/MIA/reading/diffusion.gaussian.kernel.pdf.pdf
        vec4 textureValue = vec4(0.0);
        float twoSigmaSquare = 2.0 * sigma * sigma;
        float twoPiSigmaSquare = PI * twoSigmaSquare;
        for (int x = -kernelSizeDiv2; x <= kernelSizeDiv2; x++) {
            for (int y = -kernelSizeDiv2; y <= kernelSizeDiv2; y++){
                float weight = exp(-(float(x*x + y*y) / twoSigmaSquare)) / twoPiSigmaSquare;
                ivec2 clamped = clampCoord(ivec2(centerX + x, centerY + y), sizeX, sizeY);
                textureValue += texelFetch( image, clamped, 0 ) * weight;
            }
        }
        return textureValue;
    }

    vec4 laplacianFilter(int centerX, int centerY, int sizeX, int sizeY){
        // https://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm

        // Kernel of Laplacian filter
        // [[0  -1  0]
        //  [-1  4 -1]
        //  [0  -1  0]]
        vec4 textureValue = vec4(0.0);
        textureValue += texelFetch( image, clampCoord(ivec2(centerX + 0, centerY + 0), sizeX, sizeY), 0 ) * 4.0;
        textureValue -= texelFetch( image, clampCoord(ivec2(centerX + 1, centerY + 0), sizeX, sizeY), 0 );
        textureValue -= texelFetch( image, clampCoord(ivec2(centerX - 1, centerY + 0), sizeX, sizeY), 0 );
        textureValue -= texelFetch( image, clampCoord(ivec2(centerX + 0, centerY + 1), sizeX, sizeY), 0 );
        textureValue -= texelFetch( image, clampCoord(ivec2(centerX + 0, centerY - 1), sizeX, sizeY), 0 );
        return textureValue * laplacianFactor;

    }

    vec4 separableGaussianFilterVertical(int centerX, int centerY, int sizeX, int sizeY){
        
        // https://pages.stat.wisc.edu/~mchung/teaching/MIA/reading/diffusion.gaussian.kernel.pdf.pdf 3.6 Separability
        vec4 textureValue = vec4(0.0);
        float sigmaSquare = sigma * sigma;
        float twoSigmaSquare = 2.0 * sigmaSquare;
        float sqrt2PiSigma = sqrt(2.0 * PI * sigmaSquare);

        for (int kernelIdx = -kernelSizeDiv2; kernelIdx <= kernelSizeDiv2; kernelIdx++) {
            float weight = exp(-(float(kernelIdx*kernelIdx) / twoSigmaSquare)) / sqrt2PiSigma;
            ivec2 clamped = clampCoord(ivec2(centerX, centerY + kernelIdx), sizeX, sizeY);
            textureValue += texelFetch( image, clamped, 0 ) * weight;
        }
        return textureValue;
    }

    vec4 separableGaussianFilterHorizontal(int centerX, int centerY, int sizeX, int sizeY){
        // https://pages.stat.wisc.edu/~mchung/teaching/MIA/reading/diffusion.gaussian.kernel.pdf.pdf 3.6 Separability
        vec4 textureValue = vec4(0.0);
        float sigmaSquare = sigma * sigma;
        float twoSigmaSquare = 2.0 * sigmaSquare;
        float sqrt2PiSigma = sqrt(2.0 * PI * sigmaSquare);

        for (int kernelIdx = -kernelSizeDiv2; kernelIdx <= kernelSizeDiv2; kernelIdx++) {
            float weight = exp(-(float(kernelIdx*kernelIdx) / twoSigmaSquare)) / sqrt2PiSigma;
            ivec2 clamped = clampCoord(ivec2(centerX + kernelIdx, centerY ), sizeX, sizeY);
            textureValue += texelFetch( image, clamped, 0 ) * weight;
        }
        return textureValue;
    }

    vec4 medianFilter(int centerX, int centerY, int sizeX, int sizeY){

        int kernelSizeDiv2 = min(kernelSizeDiv2, 3);
        ivec2 clamped;
        vec3 color;
        vec3 colors[MEDIAN_KERNEL_SIZE];
        float values[MEDIAN_KERNEL_SIZE];
        int index = 0;

        for (int i = -kernelSizeDiv2; i <= kernelSizeDiv2; i++) {
            for (int j = -kernelSizeDiv2; j <= kernelSizeDiv2; j++) {
                clamped = clampCoord(ivec2(centerX + i, centerY + j), sizeX, sizeY);
                color = texelFetch(image, clamped, 0).rgb;
                colors[index] = color;
                values[index] = length(color);
                index++;
            }
        }

        // insertion sort
        vec3 col;
        float key;
        for (int i = 1; i < MEDIAN_KERNEL_SIZE; ++i) {
            key = values[i];
            col = colors[i];
            int j = i - 1;
            while (j >= 0 && values[j] > key) {
                values[j + 1] = values[j];
                colors[j + 1] = colors[j];
                j--;
            }
            values[j + 1] = key;
            colors[j + 1] = col;
        }
        return vec4(colors[index / 2], 1);
    }


    void main(void) {
        ivec2 texSize2d = textureSize(image, 0);

        int x = int(gl_FragCoord.x);
        int y = int(gl_FragCoord.y);

        x = x % (texSize2d.x/2);

        // FILTER is macro defined at /src/shader.js
        vec4 textureValue = FILTER(int(gl_FragCoord.x), int(gl_FragCoord.y), texSize2d.x, texSize2d.y);

        out_FragColor = textureValue;
        out_FragColor.a = 1.0;
    }

  </script>

  <script id="anaglyphFragmentShader" type="x-shader/x-fragment">
    precision highp float;

    uniform int sizeDiv2;
    uniform sampler2D image;
    uniform float scale;
    uniform float translateX;
    uniform float translateY;

    out vec4 out_FragColor;

    vec3 trueAnaglyph(vec3 left, vec3 right){
        vec3 color = vec3(0.0);
        color.r = 0.299 * left.r + 0.587 * left.g + 0.114 * left.b;
        color.b = 0.299 * right.r + 0.587 * right.g + 0.114 * right.b;
        return color;
    }


    vec3 grayAnaglyph(vec3 left, vec3 right){
        vec3 color = vec3(0.0);
        color.r = 0.299 * left.r + 0.587 * left.g + 0.114 * left.b;
        color.g = 0.299 * right.r + 0.587 * right.g + 0.114 * right.b;
        color.b = 0.299 * right.r + 0.587 * right.g + 0.114 * right.b;
        return color;
    }

    vec3 colorAnaglyph(vec3 left, vec3 right) {
        vec3 color = vec3(0.0);
        color.r = left.r;
        color.g = right.g;
        color.b = right.b;
        return color;
    }

    vec3 halfColorAnaglyph(vec3 left, vec3 right) {
        vec3 color = vec3(0.0);
        color.r = 0.299 * left.r + 0.587 * left.g + 0.114 * left.b;
        color.g = right.g;
        color.b = right.b;
        return color;
    }

    vec3 optimizedAnaglyph(vec3 left, vec3 right) {
        vec3 color = vec3(0.0);
        color.r = 0.7 * left.r + 0.3 * left.r;
        color.g = right.g;
        color.b = right.b;
        return color;
    }


    void main(void) {
        ivec2 texSize2d = textureSize(image, 0);

        // Apply translation
        int leftX = int((gl_FragCoord.x + translateX * float(texSize2d.x))* scale ) % (texSize2d.x / 2);
        int leftY = int((gl_FragCoord.y + translateY * float(texSize2d.y))* scale ) % texSize2d.y;

        vec4 leftTextureValue = texelFetch(image, ivec2(leftX, leftY), 0 );

        int rightX = leftX + texSize2d.x / 2;
        int rightY = leftY;
        vec4 rightTextureValue = texelFetch(image, ivec2(rightX, rightY), 0 );

        // Color Anaglyphs
        out_FragColor.a = 1.0;
        // ANAGLYPH is macro defined at /src/shader.js
        out_FragColor.rgb = ANAGLYPH(leftTextureValue.rgb, rightTextureValue.rgb);

    }


  </script>

  <script type="importmap">
      {
        "imports": {
          "three": "https://unpkg.com/three@0.172.0/build/three.module.js",
          "three/addons/": "https://unpkg.com/three@0.172.0/examples/jsm/",
          "three/src/math/MathUtils": "https://unpkg.com/three@0.172.0/src/math/MathUtils.js",
          "three/examples/jsm/": "https://unpkg.com/three@0.172.0/examples/jsm/"
        }
      }
    </script>
  <script type="module">
    import * as THREE from "https://unpkg.com/three@0.172.0/build/three.module.js"
    import WEBGL from 'three/examples/jsm/capabilities/WebGL.js'
    import { OrbitControls } from 'three/examples/jsm/controls/OrbitControls.js'
    import { GUI } from 'three/examples/jsm/libs/lil-gui.module.min.js'


    // load shaders
    const defaultVertexShader = document.getElementById("defaultVertexShader").textContent
    const anaglyphFragmentShader = document.getElementById("anaglyphFragmentShader").textContent
    const filterFragmentShader = document.getElementById("filterFragmentShader").textContent

    const anaglyphMacros = {
      trueAnaglyphs: { 'ANAGLYPH(left, right)': 'trueAnaglyph(left, right)' },
      colorAnaglyphs: { 'ANAGLYPH(left, right)': 'colorAnaglyph(left, right)' },
      grayAnaglyphs: { 'ANAGLYPH(left, right)': 'grayAnaglyph(left, right)' },
      halfColorAnaglyphs: {
        'ANAGLYPH(left, right)': 'halfColorAnaglyph(left, right)',
      },
      optimizedAnaglyphs: {
        'ANAGLYPH(left, right)': 'optimizedAnaglyph(left, right)',
      },
    }

    const anaglyphMacrosName = Object.keys(anaglyphMacros)

    const filterMacros = {
      'No Filter': null,
      gaussianFilter: {
        'FILTER(X, Y, sizeX, sizeY)': 'gaussianFilter(X, Y, sizeX, sizeY)',
      },
      laplacianFilter: {
        'FILTER(X, Y, sizeX, sizeY)': 'laplacianFilter(X, Y, sizeX, sizeY)',
      },
      separableGaussianFilterHorizontal: {
        'FILTER(X, Y, sizeX, sizeY)': 'separableGaussianFilterHorizontal(X, Y, sizeX, sizeY)',
      },
      separableGaussianFilterVertical: {
        'FILTER(X, Y, sizeX, sizeY)': 'separableGaussianFilterVertical(X, Y, sizeX, sizeY)',
      },
      medianFilter: {
        'FILTER(X, Y, sizeX, sizeY)': 'medianFilter(X, Y, sizeX, sizeY)',
      },
    }

    const filterMacrosName = [
      'No Filter',
      'gaussianFilter',
      'laplacianFilter',
      'separableGaussianFilter',
      'medianFilter',
      'Gaussian Filter + Laplacian Filter',
    ]

    /**
     * Utility functions for the application
     * Contains helper functions for Three.js scene management and event handling
     */

    /**
     * Creates a window resize event handler that updates camera and renderer when the window is resized
     * This ensures the scene maintains the correct aspect ratio and fills the viewport
     *
     * @param {THREE.Camera} camera - The camera to adjust aspect ratio for
     * @param {THREE.WebGLRenderer} renderer - The renderer to adjust size for
     * @returns {Function} A resize handler function that can be used as an event listener
     */
    const createWindowResizeHandler = (camera, renderer) =>
    {
      const handleResize = () =>
      {
        camera.aspect = window.innerWidth / window.innerHeight
        camera.updateProjectionMatrix()

        renderer.setSize(window.innerWidth, window.innerHeight)
      }
      return handleResize
    }

    /**
     * Removes an object from the scene by its name if it exists
     * Useful for cleaning up scene objects before replacing them
     *
     * @param {THREE.Scene} scene - The scene to remove the object from
     * @param {string} name - The name of the object to be removed
     * @returns {void}
     */
    const removeObjectByName = (scene, name) =>
    {
      const targetObject = scene.getObjectByName(name)
      if (targetObject) {
        scene.remove(targetObject)
        console.log('Object removed:', name)
      }
    }

    /**
     * Rendering Pipeline Pass Module
     * Creates an individual rendering pass for processing images with shaders
     * Each pass takes an input texture, applies a shader, and renders to a target
     */

    class RenderingPipelinePass
    {
      /**
       * Creates a rendering pass with defined shaders and settings
       * @param {THREE.Texture} sourceTexture - Input texture for the pass
       * @param {number} targetWidth - Width of the render target
       * @param {number} targetHeight - Height of the render target
       * @param {Object} uniforms - Shader uniforms
       * @param {Object} defines - Shader preprocessor defines
       * @param {string} fragmentShader - GLSL fragment shader code
       */
      constructor(sourceTexture, targetWidth, targetHeight, uniforms, defines, fragmentShader)
      {
        // Initialize scene and camera for this render pass
        this.scene = new THREE.Scene()
        this.camera = new THREE.OrthographicCamera(-1, 1, 1, -1, 1 / Math.pow(2, 53), 1)

        // Configure render target with appropriate settings for image processing
        const options = {
          minFilter: THREE.NearestFilter,
          magFilter: THREE.NearestFilter,
          format: THREE.RGBAFormat,
          type: THREE.FloatType,
        }
        this.renderTarget = new THREE.WebGLRenderTarget(targetWidth, targetHeight, options)

        // Set up uniforms including the source texture
        this.uniforms = { ...uniforms }
        this.uniforms['image'] = { type: 't', value: sourceTexture }

        this.defines = defines
        this.fragmentShader = fragmentShader

        // Create the rendering components
        this._buildPipeline()
      }

      /**
       * Get the output texture from this render pass
       * @returns {THREE.Texture} The rendered texture
       */
      getTexture()
      {
        return this.renderTarget.texture
      }

      /**
       * Create the internal scene with a shader material and plane
       * @private
       */
      _buildPipeline()
      {
        // Create a material using the provided shaders and defines
        const imageProcessingMaterial = new THREE.RawShaderMaterial({
          uniforms: this.uniforms,
          vertexShader: defaultVertexShader,
          fragmentShader: this.fragmentShader,
          glslVersion: THREE.GLSL3,
          defines: this.defines,
        })

        // Create a full-screen plane to render the shader effect
        const positions = new Float32Array([-1, -1, 0, 1, -1, 0, 1, 1, 0, -1, -1, 0, 1, 1, 0, -1, 1, 0])
        const positionsAttribute = new THREE.BufferAttribute(positions, 3)
        const geom = new THREE.BufferGeometry()
        geom.setAttribute('position', positionsAttribute)
        const plane = new THREE.Mesh(geom, imageProcessingMaterial)
        this.scene.add(plane)
      }

      /**
       * Execute the render pass
       * @param {THREE.WebGLRenderer} renderer - The WebGL renderer
       */
      render(renderer)
      {
        renderer.setRenderTarget(this.renderTarget)
        renderer.render(this.scene, this.camera)
        renderer.setRenderTarget(null)
      }
    }


    /**
     * ImageProcessing class handles the processing pipeline for creating anaglyph effects.
     * It applies filters to video textures and creates a 3D anaglyph representation.
     */
    class ImageProcessing
    {
      /**
       * Creates a new image processing pipeline with filters and anaglyph effect
       *
       * @param {THREE.Texture} sourceTexture - The source video texture to process
       * @param {Object} uniforms - Shader uniforms for image processing
       * @param {Array<Object|null>} filterDefinesList - List of shader macro definitions for filters
       * @param {Object} anaglyphDefine - Shader macro definition for anaglyph effect
       * @param {Object} videoConfig - Configuration parameters for the video
       * @param {number} videoConfig.width - Width of the video
       * @param {number} videoConfig.height - Height of the video
       * @param {number} videoConfig.widthFactor - Width scaling factor for display
       * @param {number} videoConfig.heightFactor - Height scaling factor for display
       */
      constructor(sourceTexture, uniforms, filterDefinesList, anaglyphDefine, videoConfig)
      {
        this.sourceTexture = sourceTexture
        this.uniforms = uniforms
        this.videoConfig = videoConfig

        const sourceVideoWidth = this.videoConfig.width
        const sourceVideoHeight = this.videoConfig.height

        // Anaglyph requires splitting the image into left and right views
        this.targetWidth = sourceVideoWidth / 2
        this.targetHeight = sourceVideoHeight

        this.filterPipeline = []

        // Build the filter pipeline by sequentially applying each filter
        let texture = sourceTexture
        filterDefinesList.forEach((define) =>
        {
          console.log('filterDefine:', define)
          if (define === null) {
            console.log('define is null')
            return
          }
          const pipelinePass = new RenderingPipelinePass(
            texture,
            sourceVideoWidth,
            sourceVideoHeight,
            this.uniforms,
            define,
            filterFragmentShader,
          )
          this.filterPipeline.push(pipelinePass)
          texture = pipelinePass.getTexture()
        })
        console.log('Length of filterPipeline:', this.filterPipeline.length)

        // Create the final anaglyph processing pass
        const anaglyphPath = new RenderingPipelinePass(
          texture,
          this.targetWidth,
          this.targetHeight,
          this.uniforms,
          anaglyphDefine,
          anaglyphFragmentShader,
        )
        this.anaglyphPath = anaglyphPath
      }

      /**
       * Creates a Three.js plane with the processed video texture
       *
       * @returns {THREE.Mesh} A plane mesh with the processed video texture applied
       */
      createVideoPlane()
      {
        const hf = this.videoConfig.heightFactor
        const wf = this.videoConfig.widthFactor

        // Calculate aspect ratio based on the video dimensions and factors
        const aspectRatio = (this.targetHeight * hf) / (this.targetWidth * wf)

        const lastTexture = this.anaglyphPath.getTexture()

        // Create a plane for displaying the processed video
        const geometry = new THREE.PlaneGeometry(1, aspectRatio)
        const material = new THREE.MeshBasicMaterial({
          map: lastTexture,
          side: THREE.FrontSide,
        })
        const plane = new THREE.Mesh(geometry, material)
        plane.receiveShadow = false
        plane.castShadow = false
        return plane
      }

      /**
       * Renders the complete processing pipeline
       *
       * @param {THREE.WebGLRenderer} renderer - The WebGL renderer used to render the pipeline
       */
      render(renderer)
      {
        // First render all filters in the pipeline
        this.filterPipeline.forEach((pipeline) =>
        {
          pipeline.render(renderer)
        })
        // Then render the anaglyph effect
        this.anaglyphPath.render(renderer)
      }
    }

    /**
     * Image Processing Material Controller Module
     * Controls the image processing pipeline, manages filter application,
     * and updates the display of processed and original videos
     */

    class ImageProcessingMaterialController
    {
      /**
       * Creates a controller for managing image processing operations on video
       * @param {THREE.Scene} scene
       * @param {VideoController} videoController
       */
      constructor(scene, videoController, nbFilter)
      {
        this.scene = scene

        /** @type {VideoController} */
        this.videoController = videoController
        this.nbFilter = nbFilter

        // Initialize with the first anaglyph method
        const selectedAnaglyph = Object.keys(anaglyphMacros)[0]
        this.anaglyphDefine = anaglyphMacros[selectedAnaglyph]

        // Initialize with the first filter method
        const defaultFilterName = Object.keys(filterMacros)[0]

        // Array for holding filter defines (up to two filters can be applied)
        this.filterDefinesList = new Array(this.nbFilter)
        for (let i = 0; i < this.nbFilter; i++) {
          this.filterDefinesList[i] = filterMacros[defaultFilterName]
        }

        // Initialize image planes
        this.imageObjectProcessed = null
        this.imageObjectOriginal = null

        // Setup uniform values for shaders
        this.uniforms = {
          scale: { type: 'f', value: 1.0 },
          translateX: { type: 'f', value: 0.0 },
          translateY: { type: 'f', value: 0.0 },
          kernelSizeDiv2: { type: 'i', value: 3 },
          sigma: { type: 'f', value: 0.85 },
          laplacianFactor: { type: 'f', value: 0.5 },
        }

        // Initialize with default video
        this.init(videoController.defaultVideoName)
      }

      init(videoName)
      {
        this.onVideoChange(videoName)
      }

      /**
       * Updates the processed video display with current filters applied
       */
      updateProcessedPlane()
      {
        const name = 'videoPlaneProcessed'
        const posY = -this.videoController.getVideoConfig().posY
        // Apply selected filters

        const imageProcessing = this._createPlane(name, posY, this.filterDefinesList)
        // Store the created image processing instance for later rendering
        this.imageObjectProcessed = imageProcessing
      }

      /**
       * Updates the original video display with only anaglyph effect
       * Creates a new plane with the original video texture using only the anaglyph effect without filters
       * This provides a comparison view to the processed version
       */
      updateOriginalPlane()
      {
        const name = 'videoPlaneOriginal'
        const posY = this.videoController.getVideoConfig().posY
        // Don't apply any filter to the original video
        const filterDefinesList = []

        const imageProcessing = this._createPlane(name, posY, filterDefinesList)
        this.imageObjectOriginal = imageProcessing
      }

      /**
       * Creates a video plane with specified filters and position
       * @private
       * @param {string} name - Unique name for the mesh in the scene
       * @param {number} posY - Y position of the plane in the scene
       * @param {Array<Object|null>} filterDefinesList - List of shader defines for filters to apply
       * @returns {ImageProcessing} The created image processing instance
       */
      _createPlane(name, posY, filterDefinesList)
      {
        // Remove any existing plane with this name to avoid duplicates
        removeObjectByName(this.scene, name)

        // Create a new image processing instance with the specified filters
        const imageProcessing = new ImageProcessing(
          this.videoController.getVideoTexture(),
          this.uniforms,
          filterDefinesList,
          this.anaglyphDefine,
          this.videoController.getVideoConfig(),
        )

        // Get the rendered plane and configure its properties
        const plane = imageProcessing.createVideoPlane()
        plane.name = name
        plane.position.y = posY
        this.scene.add(plane)

        return imageProcessing
      }

      /**
       * Handles video change event
       * Loads the new video and updates both the original and processed planes
       * @async
       * @param {string} videoName - Name of the video to switch to
       * @returns {Promise<void>}
       */
      async onVideoChange(videoName)
      {
        if (this.videoController.videoName === videoName) return

        console.log('videoName', videoName)
        await this.videoController.setVideo(videoName)

        this.sourceTexture = this.videoController.getVideoTexture()
        this.updateOriginalPlane()
        this.updateProcessedPlane()
      }

      /**
       * Handles anaglyph method change event
       * @param {string} value - Name of the anaglyph method to use
       */
      onAnaglyphChange(value)
      {
        const anaglyphDefine = anaglyphMacros[value]
        if (this.anaglyphDefine === anaglyphDefine) return

        this.anaglyphDefine = anaglyphDefine
        this.updateOriginalPlane()
        this.updateProcessedPlane()
      }

      /**
       * Handles filter change event
       * @param {string} selectedFilter - Name of the filter to apply
       * @param {number} filterIdx - Index of the filter slot (0 or 1)
       */
      onFilterChange(selectedFilter, filterIdx)
      {
        const filterDefine = filterMacros[selectedFilter]
        // Don't update if the same filter is selected
        if (this.filterDefinesList[filterIdx] === filterDefine) return

        this.filterDefinesList[filterIdx] = filterDefine
        this.updateProcessedPlane()
      }

      setSeparatableFilter(filterIdx)
      {
        if (filterIdx + 1 >= this.filterDefinesList.length)
          throw new Error('Invalid position of separable filter')
        this.filterDefinesList[filterIdx] = filterMacros.separableGaussianFilterHorizontal
        this.filterDefinesList[filterIdx + 1] = filterMacros.separableGaussianFilterVertical
        this.updateProcessedPlane()
      }

      /**
       * Renders both original and processed video planes
       * @param {THREE.WebGLRenderer} renderer - WebGL renderer
       */
      render(renderer)
      {
        if (this.videoController.ready()) {
          if (this.imageObjectProcessed) this.imageObjectProcessed.render(renderer)
          if (this.imageObjectOriginal) this.imageObjectOriginal.render(renderer)
        }
      }
    }


    // Configuration for video files, including dimensions and paths
    const videoConfig = {
      'moon.mp4': {
        height: null,
        width: null,
        heightFactor: 1,
        widthFactor: 1,
        posY: 0.6,
        path: 'https://8gau8.github.io/realtime-cv-pw1/gh-pages/assets/moon.mp4',
      },
      'sf.mp4': {
        height: null,
        width: null,
        heightFactor: 1,
        widthFactor: 2,
        posY: 0.3,
        path: 'https://8gau8.github.io/realtime-cv-pw1/gh-pages/assets/sf.mp4',
      },
    }

    // VideoController class manages video playback and texture creation
    class VideoController
    {
      constructor()
      {
        this.defaultVideoName = 'sf.mp4' // Default video to load
        this.videoName = null // Currently loaded video name
        this.video = null // HTML video element
        this.videoNames = Object.keys(videoConfig) // List of available video names
      }

      // Load the video and resolve when it's ready
      async load()
      {
        return new Promise((resolve, reject) =>
        {
          if (!this.video) {
            reject(new Error('Video not initialized'))
            return
          }

          this.video.onloadeddata = () =>
          {
            resolve(this.video)
            this.video.play()
          }
          this.video.onerror = (e) =>
          {
            reject(new Error('Failed to load video: ' + e.message))
          }
        })
      }

      // Set a new video by name and load it
      async setVideo(videoName)
      {
        if (this.videoName === videoName) {
          console.log('video reload')
          return
        }
        this.videoName = videoName

        this.video = document.createElement('video')
        this.video.src = videoConfig[this.videoName].path
        this.video.preload = 'auto'
        this.video.load()
        this.video.muted = true
        this.video.loop = true

        await this.load()
        videoConfig[this.videoName].height = this.video.videoHeight
        videoConfig[this.videoName].width = this.video.videoWidth
      }

      /**
       * Creates a Three.js VideoTexture from the video element
       * Configures texture parameters for optimal rendering
       * @returns {THREE.VideoTexture} Configured video texture ready for use in materials
       */
      getVideoTexture()
      {
        const videoTexture = new THREE.VideoTexture(this.video)
        videoTexture.minFilter = THREE.NearestFilter
        videoTexture.magFilter = THREE.NearestFilter
        videoTexture.generateMipmaps = false
        videoTexture.format = THREE.RGBAFormat
        return videoTexture
      }

      /**
       * Returns the configuration object for the currently loaded video
       * @returns {Object} videoConfig - Video configuration containing dimensions and position settings
       * @returns {number} videoConfig.width - Width of the video
       * @returns {number} videoConfig.height - Height of the video
       * @returns {number} videoConfig.widthFactor - Width scaling factor for display
       * @returns {number} videoConfig.heightFactor - Height scaling factor for display
       *
       */
      getVideoConfig()
      {
        return videoConfig[this.videoName]
      }

      /**
       * Toggles the play/pause state of the video
       * If video is paused, it will play, and vice versa
       * @returns {void}
       */
      togglePlayPause()
      {
        if (!this.video) return

        if (this.video.paused) {
          console.log('play')
          this.video.play()
        } else {
          console.log('pause')
          this.video.pause()
        }
      }

      /**
       * Adjusts the current playback time of the video
       * @param {number} timeDelta - Number of seconds to add (positive) or subtract (negative)
       * @returns {void}
       */
      adjustVideoTime(timeDelta)
      {
        if (!this.video) return

        const updatedTime = Math.min(
          Math.max(this.video.currentTime + timeDelta, 0),
          this.video.duration,
        )
        this.video.currentTime = updatedTime
      }

      /**
       * Check if the video is ready for playback
       * @returns {boolean} True if the video is ready, false otherwise
       */
      ready()
      {
        return this.video !== null
      }
    }

    /**
     * Initialization utilities
     * This module handles the setup of the WebGL renderer and camera for the application
     */

    /**
     * Initializes the WebGL2 renderer
     * Creates the necessary DOM elements and configures the renderer
     * @returns {THREE.WebGLRenderer|boolean} The configured renderer or false if WebGL2 is not available
     */
    const initRenderer = () =>
    {
      if (WEBGL.isWebGL2Available() === false) {
        document.body.appendChild(WEBGL.getWebGL2ErrorMessage())
        return false
      }

      // Create container for renderer
      const container = document.createElement('div')
      document.body.appendChild(container)

      // Setup WebGL2 canvas and context
      const canvas = document.createElement('canvas')
      const context = canvas.getContext('webgl2')
      document.body.appendChild(canvas)

      // Configure renderer with WebGL2 support
      const renderer = new THREE.WebGLRenderer({
        canvas: canvas,
        context: context,
      })
      renderer.autoClear = false
      renderer.setPixelRatio(window.devicePixelRatio)
      renderer.setSize(window.innerWidth, window.innerHeight)
      renderer.shadowMap.enabled = false

      container.appendChild(renderer.domElement)
      return renderer
    }

    /**
     * Initializes the camera for the scene
     * Creates a perspective camera with appropriate positioning
     * @returns {THREE.PerspectiveCamera} The configured camera
     */
    const initCamera = () =>
    {
      const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.001, 10)
      camera.position.z = 1.5

      return camera
    }


    /**
     * GUI Initialization Module
     * Sets up the user interface controls for manipulating video and image processing parameters
     */

    /**
     * Initialize the GUI for controlling video and image processing
     * @param {GUI} gui - GUI instance
     * @param {ImageProcessingMaterialController} materialCtrl - Image processing controller
     * @param {VideoController} videoCtrl - Video controller
     */
    function initGUI(gui, materialCtrl, videoCtrl)
    {
      // Video control GUI
      const videoFolder = gui.addFolder('Video Control')

      // Add play/pause button
      videoFolder
        .add(
          {
            pausePlay: () => videoCtrl.togglePlayPause(),
          },
          'pausePlay',
        )
        .name('Pause/Play video')

      // Add skip forward button
      videoFolder
        .add(
          {
            add10sec: () => videoCtrl.adjustVideoTime(+10),
          },
          'add10sec',
        )
        .name('Add 10 seconds')

      videoFolder
        .add(
          {
            sub10sec: () => videoCtrl.adjustVideoTime(-10),
          },
          'sub10sec',
        )
        .name('Back 10 seconds')

      // Add video selection dropdown
      videoFolder
        .add({ video: videoCtrl.defaultVideoName }, 'video', videoCtrl.videoNames)
        .name('Video')
        .onChange((videoName) =>
        {
          materialCtrl.onVideoChange(videoName)
        })

      // Transform parameters controls
      const paramFolder = gui.addFolder('Transform')
      paramFolder.add(materialCtrl.uniforms.scale, 'value', 0.1, 10).name('Scale')
      paramFolder.add(materialCtrl.uniforms.translateX, 'value', 0, 1).name('Translate X')
      paramFolder.add(materialCtrl.uniforms.translateY, 'value', 0, 1).name('Translate Y')

      // Filter parameters controls
      const filterParamFolder = gui.addFolder('Filter Parameters')
      filterParamFolder
        .add(materialCtrl.uniforms.kernelSizeDiv2, 'value', 1, 20, 1)
        .name('Kernel Size / 2')
      filterParamFolder.add(materialCtrl.uniforms.sigma, 'value', 0.1, 10).name('Sigma')
      filterParamFolder.add(materialCtrl.uniforms.laplacianFactor, 'value', 0.1, 30).name('Laplacian Factor')

      // Anaglyph method selection folder
      const anaglyphGUI = gui.addFolder('Anaglyph')
      anaglyphGUI
        .add({ value: anaglyphMacrosName[0] }, 'value', anaglyphMacrosName)
        .name('Method')
        .onChange((name) =>
        {
          materialCtrl.onAnaglyphChange(name)
        })

      // First filter selection folder
      const filtersFolder = gui.addFolder('Filters')
      filtersFolder
        .add({ value: filterMacrosName[0] }, 'value', filterMacrosName)
        .name('Filter')
        .onChange((name) =>
        {
          if (name === 'separableGaussianFilter') {
            materialCtrl.setSeparatableFilter(0)
          } else if (name === 'Gaussian Filter + Laplacian Filter') {
            materialCtrl.onFilterChange('gaussianFilter', 0)
            materialCtrl.onFilterChange('laplacianFilter', 1)
          } else {
            materialCtrl.onFilterChange(name, 0)
          }
        })
    }


    /**
     * Main Application Entry Point
     * Initializes the 3D scene, controllers, and animation loop
     */

    async function main()
    {
      // Initialize Three.js components
      const renderer = initRenderer()
      const camera = initCamera()
      const scene = new THREE.Scene()

      // Set up orbit controls for interactive camera movement
      const orbitControls = new OrbitControls(camera, renderer.domElement)

      // Handle window resize
      window.addEventListener('resize', createWindowResizeHandler(camera, renderer), false)

      // Initialize video controller
      const videoController = new VideoController()

      // Initialize material controller for image processing
      const nbFilter = 2
      const materialController = new ImageProcessingMaterialController(scene, videoController, nbFilter)

      // Setup GUI controls
      const rootGui = new GUI()
      initGUI(rootGui, materialController, videoController)

      /**
       * Animation loop that runs continuously for rendering
       */
      function animate()
      {
        requestAnimationFrame(animate)
        orbitControls.update()
        renderer.clear()
        materialController.render(renderer)
        renderer.render(scene, camera)
      }

      // Start animation loop
      animate()
    }

    main()

  </script>
</body>

</html>